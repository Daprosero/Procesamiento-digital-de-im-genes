{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["**The idea is build a CNN for clasificate images of the dataset \"cifar10\", the which content cars, boats, airplanes, etc.**"],"metadata":{"id":"ZRvHV0vylWog"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"5QiXW-jfjfc8","executionInfo":{"status":"ok","timestamp":1725027116489,"user_tz":300,"elapsed":3719,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"outputs":[],"source":["#Packages:\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n","from tensorflow.keras.datasets import cifar10\n","\n","#Packages from optimizing the model:\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras import optimizers\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["#Load the data:\n","(x_train, y_train), (x_test, y_test)=cifar10.load_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8L7artIZkyKh","outputId":"eeb42e9d-311b-46e2-fad9-e8ba41eb0888","executionInfo":{"status":"ok","timestamp":1725027126743,"user_tz":300,"elapsed":8240,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"]}]},{"cell_type":"code","source":["x_train.shape #I have 50000 images of train where every one have resolution of 32x32 and is in RGB space color"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVoWe2dMk80G","outputId":"2e71d4e8-4ff9-4a75-99e1-2b63069f37d4","executionInfo":{"status":"ok","timestamp":1725027126744,"user_tz":300,"elapsed":12,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50000, 32, 32, 3)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#Painting a image:\n","plt.imshow(x_train[5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"CX8mNglolLFD","outputId":"c81d7485-4bfe-4dd1-f3cb-f28a3a9e6ede","executionInfo":{"status":"ok","timestamp":1725027126991,"user_tz":300,"elapsed":258,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7e2e5f95e3e0>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["#Preprocessing:"],"metadata":{"id":"CJKWpsMNloj7"}},{"cell_type":"code","source":["x_train=x_train.astype(\"float32\")\n","x_test=x_test.astype(\"float32\")/255"],"metadata":{"id":"lqD_jFgXlmII","executionInfo":{"status":"ok","timestamp":1725027130372,"user_tz":300,"elapsed":272,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["len(np.unique(y_train)) #I have only 10 classes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fw0zvqpomTEo","outputId":"ba66deb8-3745-41c1-ec65-685ab0371607","executionInfo":{"status":"ok","timestamp":1725027130622,"user_tz":300,"elapsed":2,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Binarizing the labels:\n","y_train=to_categorical(y_train,10)\n","y_test=to_categorical(y_test,10)"],"metadata":{"id":"fa1tzPjbma0E","executionInfo":{"status":"ok","timestamp":1725027130868,"user_tz":300,"elapsed":1,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["y_train[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dPNV0XcBmdOd","outputId":"75df6edc-6469-495f-c46e-fdd00dae3203","executionInfo":{"status":"ok","timestamp":1725027131103,"user_tz":300,"elapsed":3,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Normalizing the data lessing the mean and the standard desviation to every data (This stayes the data between 0 and 1):\n","mean=np.mean(x_train)\n","sd=np.std(x_train)\n","#I add a very samll number in the denominator for if the sd=0\n","x_train=(x_train-mean)/(sd+1e-7)\n","x_test=(x_test-mean)/(sd+1e-7) #IU normalize with the mean and the sd of the train set because the idea is that the network doesn´t know these parameters of the test set"],"metadata":{"id":"rWgawNSkLFTL","executionInfo":{"status":"ok","timestamp":1725027132378,"user_tz":300,"elapsed":935,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#Train_test_val_split:\n","(x_train,x_valid)=x_train[5000:],x_train[:5000]\n","(y_train,y_valid)=y_train[5000:],y_train[:5000]\n","\n","x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mN0jx8Pmq09","outputId":"c0102d61-426f-4b99-8279-2738b6a8b6be","executionInfo":{"status":"ok","timestamp":1725027132378,"user_tz":300,"elapsed":3,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((45000, 32, 32, 3), (45000, 10), (5000, 32, 32, 3), (5000, 10))"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#Data Augmentation:"],"metadata":{"id":"y5QLxE5hNAKb"}},{"cell_type":"code","source":["datagen=ImageDataGenerator(rotation_range=15,\n","                   width_shift_range=0.1,\n","                   height_shift_range=0.1,\n","                   horizontal_flip=True,\n","                   vertical_flip=True\n","                   )"],"metadata":{"id":"mSygtRW0NDUk","executionInfo":{"status":"ok","timestamp":1725027132378,"user_tz":300,"elapsed":2,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["#Building the model"],"metadata":{"id":"F0CfiYF5nmam"}},{"cell_type":"code","source":["base_filtros=32 #Number of Kernels in the convolutional layers\n","w_regularizer=1e-4 #weight of the relularizer"],"metadata":{"id":"TBWdxVWEnsRA","executionInfo":{"status":"ok","timestamp":1725027133192,"user_tz":300,"elapsed":1,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import Model, layers, regularizers\n","model=Sequential()\n","\n","#First convolution layer:\n","model.add(Conv2D(base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer), input_shape=(32,32,3))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","\n","#Second convolution layer:\n","model.add(Conv2D(base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu OR SIMPLY I COULD PUT A ACTIVATION FUNCTION IN THE CONVOLUTIONAL LAYER\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","model.add(MaxPooling2D(pool_size=(2,2))) #First MaxPooling Layer\n","model.add(Dropout(0.2))\n","\n","\n","#Convolution layer number 3 (Iqual but increase the number of kernels):\n","model.add(Conv2D(2*base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","model.add(Dropout(0.2))\n","\n","\n","#Convolution layer number 4:\n","model.add(Conv2D(2*base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","model.add(MaxPooling2D(pool_size=(2,2))) #Second MaxPooling Layer\n","model.add(Dropout(0.3))\n","\n","\n","\n","#Convolution layer number 5 (increase the number of kernels):\n","model.add(Conv2D(4*base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","\n","#Convolution layer number 6:\n","model.add(Conv2D(4*base_filtros,(3,3),padding=\"same\",kernel_regularizer=regularizers.l2(w_regularizer))) #put a regularizer with l2\n","model.add(Activation(\"relu\")) #Put a Activation layer of type relu\n","model.add(BatchNormalization()) #BatchNormalization Layer\n","model.add(MaxPooling2D(pool_size=(2,2))) #MaxPooling Layer Number three\n","model.add(Dropout(0.4))\n","\n","#Classifiation (with dense layer) and Flatten:\n","model.add(Flatten())\n","model.add(Dense(10,activation=\"softmax\"))\n"],"metadata":{"id":"l2b0U1BCoKmU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1725027174570,"user_tz":300,"elapsed":2190,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}},"outputId":"55896924-e5e8-4b4d-bce3-c36860bbb1f8"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]},{"cell_type":"markdown","source":["**How we can see, generally after of a convolution layer, there is a activation layer and generally after of a activation layer there is a Batch Normalization layer **"],"metadata":{"id":"ypPIhxekMjZ5"}},{"cell_type":"code","source":["#architecture of the model:\n","model.summary() #Efectly the convolution layers increase the depth of my tensor and the pooling layer decrease the resolution of my tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"e12cI59Xsj4j","outputId":"3a24fbc2-f374-43af-8dd1-6193388b0965","executionInfo":{"status":"ok","timestamp":1725027178748,"user_tz":300,"elapsed":255,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m128\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │          \u001b[38;5;34m73,856\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m147,584\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │             \u001b[38;5;34m512\u001b[0m │\n","│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m20,490\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,490</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m309,290\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">309,290</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m308,394\u001b[0m (1.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">308,394</span> (1.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["#Compile the model:\n","model.compile(metrics=[\"accuracy\"],loss=\"categorical_crossentropy\",optimizer=optimizers.Adam())"],"metadata":{"id":"YCMfdl8stPt4","executionInfo":{"status":"ok","timestamp":1725027180871,"user_tz":300,"elapsed":225,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Define the checkpoint:\n","checkpoint=ModelCheckpoint(\"best_model.keras\",verbose=1,save_best_only=True,monitor=\"val_accuracy\")"],"metadata":{"id":"nAzxbzO5PLru","executionInfo":{"status":"ok","timestamp":1725027193406,"user_tz":300,"elapsed":231,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["#Training the model including the data augmentation and a checkpoint:\n","history=model.fit(datagen.flow(x_train,y_train,batch_size=128),callbacks=[checkpoint],steps_per_epoch=x_train.shape[0]//128,epochs=120,validation_data=(x_valid,y_valid),verbose=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdAOcs5xurcK","outputId":"3d025c57-051b-42ca-d072-b7b5bb66f65b","executionInfo":{"status":"ok","timestamp":1725029364326,"user_tz":300,"elapsed":2168813,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/120\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: val_accuracy improved from -inf to 0.29520, saving model to best_model.keras\n","351/351 - 54s - 154ms/step - accuracy: 0.3229 - loss: 2.2516 - val_accuracy: 0.2952 - val_loss: 2.1138\n","Epoch 2/120\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self.gen.throw(typ, value, traceback)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2: val_accuracy improved from 0.29520 to 0.29700, saving model to best_model.keras\n","351/351 - 1s - 1ms/step - accuracy: 0.3906 - loss: 1.8768 - val_accuracy: 0.2970 - val_loss: 2.0990\n","Epoch 3/120\n","\n","Epoch 3: val_accuracy improved from 0.29700 to 0.46660, saving model to best_model.keras\n","351/351 - 61s - 173ms/step - accuracy: 0.4340 - loss: 1.7055 - val_accuracy: 0.4666 - val_loss: 1.5839\n","Epoch 4/120\n","\n","Epoch 4: val_accuracy improved from 0.46660 to 0.47260, saving model to best_model.keras\n","351/351 - 0s - 1ms/step - accuracy: 0.4688 - loss: 1.5582 - val_accuracy: 0.4726 - val_loss: 1.5658\n","Epoch 5/120\n","\n","Epoch 5: val_accuracy improved from 0.47260 to 0.52840, saving model to best_model.keras\n","351/351 - 40s - 113ms/step - accuracy: 0.4978 - loss: 1.4938 - val_accuracy: 0.5284 - val_loss: 1.4594\n","Epoch 6/120\n","\n","Epoch 6: val_accuracy did not improve from 0.52840\n","351/351 - 0s - 1ms/step - accuracy: 0.5781 - loss: 1.3384 - val_accuracy: 0.5202 - val_loss: 1.4841\n","Epoch 7/120\n","\n","Epoch 7: val_accuracy improved from 0.52840 to 0.57540, saving model to best_model.keras\n","351/351 - 40s - 115ms/step - accuracy: 0.5459 - loss: 1.3493 - val_accuracy: 0.5754 - val_loss: 1.3172\n","Epoch 8/120\n","\n","Epoch 8: val_accuracy did not improve from 0.57540\n","351/351 - 0s - 1ms/step - accuracy: 0.5547 - loss: 1.2556 - val_accuracy: 0.5754 - val_loss: 1.3460\n","Epoch 9/120\n","\n","Epoch 9: val_accuracy improved from 0.57540 to 0.62080, saving model to best_model.keras\n","351/351 - 40s - 115ms/step - accuracy: 0.5766 - loss: 1.2514 - val_accuracy: 0.6208 - val_loss: 1.2081\n","Epoch 10/120\n","\n","Epoch 10: val_accuracy improved from 0.62080 to 0.62280, saving model to best_model.keras\n","351/351 - 0s - 1ms/step - accuracy: 0.6172 - loss: 1.1235 - val_accuracy: 0.6228 - val_loss: 1.2100\n","Epoch 11/120\n","\n","Epoch 11: val_accuracy did not improve from 0.62280\n","351/351 - 28s - 79ms/step - accuracy: 0.6021 - loss: 1.1771 - val_accuracy: 0.6142 - val_loss: 1.2021\n","Epoch 12/120\n","\n","Epoch 12: val_accuracy did not improve from 0.62280\n","351/351 - 0s - 1ms/step - accuracy: 0.6875 - loss: 0.9419 - val_accuracy: 0.6130 - val_loss: 1.1994\n","Epoch 13/120\n","\n","Epoch 13: val_accuracy improved from 0.62280 to 0.63020, saving model to best_model.keras\n","351/351 - 40s - 114ms/step - accuracy: 0.6229 - loss: 1.1107 - val_accuracy: 0.6302 - val_loss: 1.1675\n","Epoch 14/120\n","\n","Epoch 14: val_accuracy did not improve from 0.63020\n","351/351 - 0s - 1ms/step - accuracy: 0.6562 - loss: 1.0545 - val_accuracy: 0.6270 - val_loss: 1.1739\n","Epoch 15/120\n","\n","Epoch 15: val_accuracy improved from 0.63020 to 0.67780, saving model to best_model.keras\n","351/351 - 28s - 79ms/step - accuracy: 0.6425 - loss: 1.0698 - val_accuracy: 0.6778 - val_loss: 0.9790\n","Epoch 16/120\n","\n","Epoch 16: val_accuracy improved from 0.67780 to 0.68280, saving model to best_model.keras\n","351/351 - 1s - 2ms/step - accuracy: 0.6719 - loss: 1.0325 - val_accuracy: 0.6828 - val_loss: 0.9622\n","Epoch 17/120\n","\n","Epoch 17: val_accuracy improved from 0.68280 to 0.70640, saving model to best_model.keras\n","351/351 - 40s - 113ms/step - accuracy: 0.6540 - loss: 1.0281 - val_accuracy: 0.7064 - val_loss: 0.9174\n","Epoch 18/120\n","\n","Epoch 18: val_accuracy improved from 0.70640 to 0.70660, saving model to best_model.keras\n","351/351 - 1s - 2ms/step - accuracy: 0.6172 - loss: 1.0616 - val_accuracy: 0.7066 - val_loss: 0.9195\n","Epoch 19/120\n","\n","Epoch 19: val_accuracy did not improve from 0.70660\n","351/351 - 40s - 114ms/step - accuracy: 0.6687 - loss: 0.9990 - val_accuracy: 0.6766 - val_loss: 1.0429\n","Epoch 20/120\n","\n","Epoch 20: val_accuracy did not improve from 0.70660\n","351/351 - 0s - 1ms/step - accuracy: 0.7188 - loss: 0.9032 - val_accuracy: 0.6768 - val_loss: 1.0337\n","Epoch 21/120\n","\n","Epoch 21: val_accuracy improved from 0.70660 to 0.70760, saving model to best_model.keras\n","351/351 - 41s - 117ms/step - accuracy: 0.6785 - loss: 0.9714 - val_accuracy: 0.7076 - val_loss: 0.9097\n","Epoch 22/120\n","\n","Epoch 22: val_accuracy did not improve from 0.70760\n","351/351 - 1s - 2ms/step - accuracy: 0.6797 - loss: 0.9166 - val_accuracy: 0.7046 - val_loss: 0.9198\n","Epoch 23/120\n","\n","Epoch 23: val_accuracy did not improve from 0.70760\n","351/351 - 40s - 114ms/step - accuracy: 0.6876 - loss: 0.9481 - val_accuracy: 0.6956 - val_loss: 0.9706\n","Epoch 24/120\n","\n","Epoch 24: val_accuracy did not improve from 0.70760\n","351/351 - 1s - 2ms/step - accuracy: 0.6953 - loss: 0.9142 - val_accuracy: 0.6940 - val_loss: 0.9641\n","Epoch 25/120\n","\n","Epoch 25: val_accuracy improved from 0.70760 to 0.73400, saving model to best_model.keras\n","351/351 - 28s - 80ms/step - accuracy: 0.6986 - loss: 0.9263 - val_accuracy: 0.7340 - val_loss: 0.8556\n","Epoch 26/120\n","\n","Epoch 26: val_accuracy did not improve from 0.73400\n","351/351 - 1s - 2ms/step - accuracy: 0.6719 - loss: 0.9441 - val_accuracy: 0.7298 - val_loss: 0.8639\n","Epoch 27/120\n","\n","Epoch 27: val_accuracy did not improve from 0.73400\n","351/351 - 40s - 114ms/step - accuracy: 0.7057 - loss: 0.9051 - val_accuracy: 0.7248 - val_loss: 0.8843\n","Epoch 28/120\n","\n","Epoch 28: val_accuracy did not improve from 0.73400\n","351/351 - 1s - 2ms/step - accuracy: 0.6797 - loss: 1.0670 - val_accuracy: 0.7210 - val_loss: 0.8865\n","Epoch 29/120\n","\n","Epoch 29: val_accuracy did not improve from 0.73400\n","351/351 - 29s - 82ms/step - accuracy: 0.7090 - loss: 0.8955 - val_accuracy: 0.7136 - val_loss: 0.9463\n","Epoch 30/120\n","\n","Epoch 30: val_accuracy did not improve from 0.73400\n","351/351 - 1s - 2ms/step - accuracy: 0.6562 - loss: 1.0151 - val_accuracy: 0.7140 - val_loss: 0.9412\n","Epoch 31/120\n","\n","Epoch 31: val_accuracy did not improve from 0.73400\n","351/351 - 27s - 78ms/step - accuracy: 0.7166 - loss: 0.8845 - val_accuracy: 0.7316 - val_loss: 0.8725\n","Epoch 32/120\n","\n","Epoch 32: val_accuracy did not improve from 0.73400\n","351/351 - 0s - 1ms/step - accuracy: 0.7344 - loss: 0.8898 - val_accuracy: 0.7340 - val_loss: 0.8566\n","Epoch 33/120\n","\n","Epoch 33: val_accuracy did not improve from 0.73400\n","351/351 - 40s - 115ms/step - accuracy: 0.7218 - loss: 0.8711 - val_accuracy: 0.7232 - val_loss: 0.8861\n","Epoch 34/120\n","\n","Epoch 34: val_accuracy did not improve from 0.73400\n","351/351 - 1s - 2ms/step - accuracy: 0.6641 - loss: 0.9875 - val_accuracy: 0.7264 - val_loss: 0.8820\n","Epoch 35/120\n","\n","Epoch 35: val_accuracy improved from 0.73400 to 0.73780, saving model to best_model.keras\n","351/351 - 28s - 79ms/step - accuracy: 0.7282 - loss: 0.8649 - val_accuracy: 0.7378 - val_loss: 0.8701\n","Epoch 36/120\n","\n","Epoch 36: val_accuracy improved from 0.73780 to 0.73900, saving model to best_model.keras\n","351/351 - 1s - 2ms/step - accuracy: 0.7422 - loss: 0.8153 - val_accuracy: 0.7390 - val_loss: 0.8717\n","Epoch 37/120\n","\n","Epoch 37: val_accuracy improved from 0.73900 to 0.74800, saving model to best_model.keras\n","351/351 - 28s - 79ms/step - accuracy: 0.7301 - loss: 0.8555 - val_accuracy: 0.7480 - val_loss: 0.8379\n","Epoch 38/120\n","\n","Epoch 38: val_accuracy did not improve from 0.74800\n","351/351 - 1s - 2ms/step - accuracy: 0.7031 - loss: 0.9328 - val_accuracy: 0.7464 - val_loss: 0.8476\n","Epoch 39/120\n","\n","Epoch 39: val_accuracy improved from 0.74800 to 0.75840, saving model to best_model.keras\n","351/351 - 27s - 78ms/step - accuracy: 0.7339 - loss: 0.8541 - val_accuracy: 0.7584 - val_loss: 0.7989\n","Epoch 40/120\n","\n","Epoch 40: val_accuracy did not improve from 0.75840\n","351/351 - 0s - 1ms/step - accuracy: 0.7266 - loss: 0.8395 - val_accuracy: 0.7564 - val_loss: 0.8073\n","Epoch 41/120\n","\n","Epoch 41: val_accuracy did not improve from 0.75840\n","351/351 - 41s - 117ms/step - accuracy: 0.7397 - loss: 0.8361 - val_accuracy: 0.7414 - val_loss: 0.8786\n","Epoch 42/120\n","\n","Epoch 42: val_accuracy did not improve from 0.75840\n","351/351 - 0s - 1ms/step - accuracy: 0.7891 - loss: 0.7515 - val_accuracy: 0.7440 - val_loss: 0.8646\n","Epoch 43/120\n","\n","Epoch 43: val_accuracy improved from 0.75840 to 0.76420, saving model to best_model.keras\n","351/351 - 29s - 82ms/step - accuracy: 0.7415 - loss: 0.8296 - val_accuracy: 0.7642 - val_loss: 0.7803\n","Epoch 44/120\n","\n","Epoch 44: val_accuracy did not improve from 0.76420\n","351/351 - 1s - 2ms/step - accuracy: 0.7578 - loss: 0.9082 - val_accuracy: 0.7616 - val_loss: 0.7897\n","Epoch 45/120\n","\n","Epoch 45: val_accuracy improved from 0.76420 to 0.78080, saving model to best_model.keras\n","351/351 - 39s - 112ms/step - accuracy: 0.7451 - loss: 0.8262 - val_accuracy: 0.7808 - val_loss: 0.7358\n","Epoch 46/120\n","\n","Epoch 46: val_accuracy did not improve from 0.78080\n","351/351 - 0s - 1ms/step - accuracy: 0.7500 - loss: 0.7980 - val_accuracy: 0.7786 - val_loss: 0.7403\n","Epoch 47/120\n","\n","Epoch 47: val_accuracy did not improve from 0.78080\n","351/351 - 40s - 114ms/step - accuracy: 0.7492 - loss: 0.8181 - val_accuracy: 0.7702 - val_loss: 0.8017\n","Epoch 48/120\n","\n","Epoch 48: val_accuracy did not improve from 0.78080\n","351/351 - 0s - 1ms/step - accuracy: 0.7031 - loss: 1.0302 - val_accuracy: 0.7696 - val_loss: 0.8038\n","Epoch 49/120\n","\n","Epoch 49: val_accuracy did not improve from 0.78080\n","351/351 - 41s - 118ms/step - accuracy: 0.7505 - loss: 0.8132 - val_accuracy: 0.7764 - val_loss: 0.7893\n","Epoch 50/120\n","\n","Epoch 50: val_accuracy did not improve from 0.78080\n","351/351 - 1s - 2ms/step - accuracy: 0.7500 - loss: 0.9015 - val_accuracy: 0.7708 - val_loss: 0.7984\n","Epoch 51/120\n","\n","Epoch 51: val_accuracy did not improve from 0.78080\n","351/351 - 27s - 77ms/step - accuracy: 0.7507 - loss: 0.8134 - val_accuracy: 0.7442 - val_loss: 0.8677\n","Epoch 52/120\n","\n","Epoch 52: val_accuracy did not improve from 0.78080\n","351/351 - 1s - 2ms/step - accuracy: 0.8047 - loss: 0.8406 - val_accuracy: 0.7442 - val_loss: 0.8663\n","Epoch 53/120\n","\n","Epoch 53: val_accuracy improved from 0.78080 to 0.79280, saving model to best_model.keras\n","351/351 - 41s - 116ms/step - accuracy: 0.7572 - loss: 0.8022 - val_accuracy: 0.7928 - val_loss: 0.7098\n","Epoch 54/120\n","\n","Epoch 54: val_accuracy did not improve from 0.79280\n","351/351 - 0s - 1ms/step - accuracy: 0.7812 - loss: 0.6649 - val_accuracy: 0.7910 - val_loss: 0.7133\n","Epoch 55/120\n","\n","Epoch 55: val_accuracy did not improve from 0.79280\n","351/351 - 28s - 79ms/step - accuracy: 0.7594 - loss: 0.7978 - val_accuracy: 0.7720 - val_loss: 0.7780\n","Epoch 56/120\n","\n","Epoch 56: val_accuracy did not improve from 0.79280\n","351/351 - 1s - 2ms/step - accuracy: 0.7500 - loss: 0.7570 - val_accuracy: 0.7738 - val_loss: 0.7786\n","Epoch 57/120\n","\n","Epoch 57: val_accuracy did not improve from 0.79280\n","351/351 - 40s - 114ms/step - accuracy: 0.7581 - loss: 0.7996 - val_accuracy: 0.7818 - val_loss: 0.7348\n","Epoch 58/120\n","\n","Epoch 58: val_accuracy did not improve from 0.79280\n","351/351 - 1s - 2ms/step - accuracy: 0.8750 - loss: 0.6385 - val_accuracy: 0.7800 - val_loss: 0.7338\n","Epoch 59/120\n","\n","Epoch 59: val_accuracy did not improve from 0.79280\n","351/351 - 40s - 114ms/step - accuracy: 0.7635 - loss: 0.7901 - val_accuracy: 0.7670 - val_loss: 0.8111\n","Epoch 60/120\n","\n","Epoch 60: val_accuracy did not improve from 0.79280\n","351/351 - 1s - 2ms/step - accuracy: 0.7500 - loss: 0.7911 - val_accuracy: 0.7708 - val_loss: 0.7989\n","Epoch 61/120\n","\n","Epoch 61: val_accuracy improved from 0.79280 to 0.79360, saving model to best_model.keras\n","351/351 - 40s - 114ms/step - accuracy: 0.7662 - loss: 0.7841 - val_accuracy: 0.7936 - val_loss: 0.7243\n","Epoch 62/120\n","\n","Epoch 62: val_accuracy did not improve from 0.79360\n","351/351 - 1s - 2ms/step - accuracy: 0.8359 - loss: 0.6012 - val_accuracy: 0.7918 - val_loss: 0.7274\n","Epoch 63/120\n","\n","Epoch 63: val_accuracy did not improve from 0.79360\n","351/351 - 28s - 80ms/step - accuracy: 0.7663 - loss: 0.7859 - val_accuracy: 0.7770 - val_loss: 0.7609\n","Epoch 64/120\n","\n","Epoch 64: val_accuracy did not improve from 0.79360\n","351/351 - 0s - 1ms/step - accuracy: 0.7500 - loss: 0.9381 - val_accuracy: 0.7774 - val_loss: 0.7644\n","Epoch 65/120\n","\n","Epoch 65: val_accuracy did not improve from 0.79360\n","351/351 - 40s - 114ms/step - accuracy: 0.7686 - loss: 0.7771 - val_accuracy: 0.7894 - val_loss: 0.7295\n","Epoch 66/120\n","\n","Epoch 66: val_accuracy did not improve from 0.79360\n","351/351 - 0s - 1ms/step - accuracy: 0.7500 - loss: 0.7817 - val_accuracy: 0.7888 - val_loss: 0.7362\n","Epoch 67/120\n","\n","Epoch 67: val_accuracy did not improve from 0.79360\n","351/351 - 27s - 77ms/step - accuracy: 0.7683 - loss: 0.7847 - val_accuracy: 0.7908 - val_loss: 0.7340\n","Epoch 68/120\n","\n","Epoch 68: val_accuracy did not improve from 0.79360\n","351/351 - 1s - 2ms/step - accuracy: 0.7812 - loss: 0.8712 - val_accuracy: 0.7912 - val_loss: 0.7288\n","Epoch 69/120\n","\n","Epoch 69: val_accuracy did not improve from 0.79360\n","351/351 - 40s - 115ms/step - accuracy: 0.7722 - loss: 0.7723 - val_accuracy: 0.7852 - val_loss: 0.7434\n","Epoch 70/120\n","\n","Epoch 70: val_accuracy did not improve from 0.79360\n","351/351 - 1s - 1ms/step - accuracy: 0.8047 - loss: 0.6791 - val_accuracy: 0.7878 - val_loss: 0.7339\n","Epoch 71/120\n","\n","Epoch 71: val_accuracy improved from 0.79360 to 0.80220, saving model to best_model.keras\n","351/351 - 28s - 79ms/step - accuracy: 0.7737 - loss: 0.7741 - val_accuracy: 0.8022 - val_loss: 0.6961\n","Epoch 72/120\n","\n","Epoch 72: val_accuracy did not improve from 0.80220\n","351/351 - 0s - 1ms/step - accuracy: 0.7422 - loss: 0.8230 - val_accuracy: 0.8018 - val_loss: 0.6932\n","Epoch 73/120\n","\n","Epoch 73: val_accuracy did not improve from 0.80220\n","351/351 - 40s - 114ms/step - accuracy: 0.7728 - loss: 0.7746 - val_accuracy: 0.7922 - val_loss: 0.7266\n","Epoch 74/120\n","\n","Epoch 74: val_accuracy did not improve from 0.80220\n","351/351 - 0s - 1ms/step - accuracy: 0.8203 - loss: 0.6458 - val_accuracy: 0.7938 - val_loss: 0.7276\n","Epoch 75/120\n","\n","Epoch 75: val_accuracy did not improve from 0.80220\n","351/351 - 27s - 78ms/step - accuracy: 0.7716 - loss: 0.7709 - val_accuracy: 0.7788 - val_loss: 0.8002\n","Epoch 76/120\n","\n","Epoch 76: val_accuracy did not improve from 0.80220\n","351/351 - 1s - 2ms/step - accuracy: 0.8281 - loss: 0.6911 - val_accuracy: 0.7752 - val_loss: 0.8057\n","Epoch 77/120\n","\n","Epoch 77: val_accuracy did not improve from 0.80220\n","351/351 - 27s - 78ms/step - accuracy: 0.7755 - loss: 0.7667 - val_accuracy: 0.7862 - val_loss: 0.7513\n","Epoch 78/120\n","\n","Epoch 78: val_accuracy did not improve from 0.80220\n","351/351 - 1s - 2ms/step - accuracy: 0.7500 - loss: 0.9315 - val_accuracy: 0.7924 - val_loss: 0.7324\n","Epoch 79/120\n","\n","Epoch 79: val_accuracy did not improve from 0.80220\n","351/351 - 28s - 79ms/step - accuracy: 0.7753 - loss: 0.7655 - val_accuracy: 0.7696 - val_loss: 0.8231\n","Epoch 80/120\n","\n","Epoch 80: val_accuracy did not improve from 0.80220\n","351/351 - 0s - 1ms/step - accuracy: 0.7578 - loss: 0.8494 - val_accuracy: 0.7706 - val_loss: 0.8140\n","Epoch 81/120\n","\n","Epoch 81: val_accuracy did not improve from 0.80220\n","351/351 - 28s - 79ms/step - accuracy: 0.7803 - loss: 0.7540 - val_accuracy: 0.7990 - val_loss: 0.7289\n","Epoch 82/120\n","\n","Epoch 82: val_accuracy did not improve from 0.80220\n","351/351 - 0s - 1ms/step - accuracy: 0.7734 - loss: 0.7705 - val_accuracy: 0.7974 - val_loss: 0.7297\n","Epoch 83/120\n","\n","Epoch 83: val_accuracy improved from 0.80220 to 0.80240, saving model to best_model.keras\n","351/351 - 27s - 78ms/step - accuracy: 0.7794 - loss: 0.7565 - val_accuracy: 0.8024 - val_loss: 0.7050\n","Epoch 84/120\n","\n","Epoch 84: val_accuracy did not improve from 0.80240\n","351/351 - 0s - 1ms/step - accuracy: 0.7578 - loss: 0.8694 - val_accuracy: 0.8012 - val_loss: 0.7070\n","Epoch 85/120\n","\n","Epoch 85: val_accuracy did not improve from 0.80240\n","351/351 - 41s - 116ms/step - accuracy: 0.7812 - loss: 0.7556 - val_accuracy: 0.7932 - val_loss: 0.7483\n","Epoch 86/120\n","\n","Epoch 86: val_accuracy did not improve from 0.80240\n","351/351 - 0s - 1ms/step - accuracy: 0.8203 - loss: 0.6636 - val_accuracy: 0.7884 - val_loss: 0.7553\n","Epoch 87/120\n","\n","Epoch 87: val_accuracy did not improve from 0.80240\n","351/351 - 28s - 81ms/step - accuracy: 0.7832 - loss: 0.7525 - val_accuracy: 0.7752 - val_loss: 0.8148\n","Epoch 88/120\n","\n","Epoch 88: val_accuracy did not improve from 0.80240\n","351/351 - 0s - 1ms/step - accuracy: 0.7734 - loss: 0.7888 - val_accuracy: 0.7770 - val_loss: 0.8039\n","Epoch 89/120\n","\n","Epoch 89: val_accuracy did not improve from 0.80240\n","351/351 - 39s - 112ms/step - accuracy: 0.7835 - loss: 0.7491 - val_accuracy: 0.7752 - val_loss: 0.8002\n","Epoch 90/120\n","\n","Epoch 90: val_accuracy did not improve from 0.80240\n","351/351 - 0s - 1ms/step - accuracy: 0.8125 - loss: 0.7449 - val_accuracy: 0.7778 - val_loss: 0.7905\n","Epoch 91/120\n","\n","Epoch 91: val_accuracy improved from 0.80240 to 0.81200, saving model to best_model.keras\n","351/351 - 41s - 116ms/step - accuracy: 0.7847 - loss: 0.7496 - val_accuracy: 0.8120 - val_loss: 0.6692\n","Epoch 92/120\n","\n","Epoch 92: val_accuracy improved from 0.81200 to 0.81220, saving model to best_model.keras\n","351/351 - 0s - 1ms/step - accuracy: 0.7344 - loss: 0.7898 - val_accuracy: 0.8122 - val_loss: 0.6729\n","Epoch 93/120\n","\n","Epoch 93: val_accuracy did not improve from 0.81220\n","351/351 - 41s - 118ms/step - accuracy: 0.7833 - loss: 0.7444 - val_accuracy: 0.8094 - val_loss: 0.6979\n","Epoch 94/120\n","\n","Epoch 94: val_accuracy did not improve from 0.81220\n","351/351 - 0s - 1ms/step - accuracy: 0.8125 - loss: 0.6055 - val_accuracy: 0.8096 - val_loss: 0.6988\n","Epoch 95/120\n","\n","Epoch 95: val_accuracy did not improve from 0.81220\n","351/351 - 39s - 112ms/step - accuracy: 0.7855 - loss: 0.7451 - val_accuracy: 0.8114 - val_loss: 0.6888\n","Epoch 96/120\n","\n","Epoch 96: val_accuracy did not improve from 0.81220\n","351/351 - 1s - 2ms/step - accuracy: 0.7344 - loss: 1.0150 - val_accuracy: 0.8112 - val_loss: 0.6912\n","Epoch 97/120\n","\n","Epoch 97: val_accuracy did not improve from 0.81220\n","351/351 - 40s - 115ms/step - accuracy: 0.7870 - loss: 0.7466 - val_accuracy: 0.7686 - val_loss: 0.8488\n","Epoch 98/120\n","\n","Epoch 98: val_accuracy did not improve from 0.81220\n","351/351 - 0s - 1ms/step - accuracy: 0.7500 - loss: 0.8887 - val_accuracy: 0.7694 - val_loss: 0.8427\n","Epoch 99/120\n","\n","Epoch 99: val_accuracy did not improve from 0.81220\n","351/351 - 42s - 119ms/step - accuracy: 0.7896 - loss: 0.7400 - val_accuracy: 0.7980 - val_loss: 0.7191\n","Epoch 100/120\n","\n","Epoch 100: val_accuracy did not improve from 0.81220\n","351/351 - 0s - 1ms/step - accuracy: 0.8359 - loss: 0.6397 - val_accuracy: 0.7976 - val_loss: 0.7143\n","Epoch 101/120\n","\n","Epoch 101: val_accuracy did not improve from 0.81220\n","351/351 - 27s - 78ms/step - accuracy: 0.7876 - loss: 0.7373 - val_accuracy: 0.7882 - val_loss: 0.7615\n","Epoch 102/120\n","\n","Epoch 102: val_accuracy did not improve from 0.81220\n","351/351 - 1s - 2ms/step - accuracy: 0.7500 - loss: 0.8106 - val_accuracy: 0.7900 - val_loss: 0.7593\n","Epoch 103/120\n","\n","Epoch 103: val_accuracy improved from 0.81220 to 0.81320, saving model to best_model.keras\n","351/351 - 40s - 114ms/step - accuracy: 0.7866 - loss: 0.7413 - val_accuracy: 0.8132 - val_loss: 0.7034\n","Epoch 104/120\n","\n","Epoch 104: val_accuracy improved from 0.81320 to 0.81380, saving model to best_model.keras\n","351/351 - 0s - 1ms/step - accuracy: 0.8125 - loss: 0.6225 - val_accuracy: 0.8138 - val_loss: 0.6983\n","Epoch 105/120\n","\n","Epoch 105: val_accuracy improved from 0.81380 to 0.81480, saving model to best_model.keras\n","351/351 - 27s - 77ms/step - accuracy: 0.7883 - loss: 0.7408 - val_accuracy: 0.8148 - val_loss: 0.6925\n","Epoch 106/120\n","\n","Epoch 106: val_accuracy did not improve from 0.81480\n","351/351 - 0s - 1ms/step - accuracy: 0.7891 - loss: 0.7619 - val_accuracy: 0.8146 - val_loss: 0.6881\n","Epoch 107/120\n","\n","Epoch 107: val_accuracy improved from 0.81480 to 0.81740, saving model to best_model.keras\n","351/351 - 41s - 116ms/step - accuracy: 0.7871 - loss: 0.7398 - val_accuracy: 0.8174 - val_loss: 0.6791\n","Epoch 108/120\n","\n","Epoch 108: val_accuracy improved from 0.81740 to 0.81760, saving model to best_model.keras\n","351/351 - 0s - 1ms/step - accuracy: 0.7500 - loss: 0.8360 - val_accuracy: 0.8176 - val_loss: 0.6765\n","Epoch 109/120\n","\n","Epoch 109: val_accuracy did not improve from 0.81760\n","351/351 - 40s - 114ms/step - accuracy: 0.7906 - loss: 0.7349 - val_accuracy: 0.8084 - val_loss: 0.7086\n","Epoch 110/120\n","\n","Epoch 110: val_accuracy did not improve from 0.81760\n","351/351 - 1s - 2ms/step - accuracy: 0.8203 - loss: 0.6674 - val_accuracy: 0.8028 - val_loss: 0.7206\n","Epoch 111/120\n","\n","Epoch 111: val_accuracy did not improve from 0.81760\n","351/351 - 31s - 87ms/step - accuracy: 0.7892 - loss: 0.7389 - val_accuracy: 0.7980 - val_loss: 0.7201\n","Epoch 112/120\n","\n","Epoch 112: val_accuracy did not improve from 0.81760\n","351/351 - 1s - 2ms/step - accuracy: 0.7578 - loss: 0.9248 - val_accuracy: 0.8016 - val_loss: 0.7141\n","Epoch 113/120\n","\n","Epoch 113: val_accuracy did not improve from 0.81760\n","351/351 - 37s - 107ms/step - accuracy: 0.7932 - loss: 0.7292 - val_accuracy: 0.8038 - val_loss: 0.7196\n","Epoch 114/120\n","\n","Epoch 114: val_accuracy did not improve from 0.81760\n","351/351 - 1s - 2ms/step - accuracy: 0.7891 - loss: 0.8280 - val_accuracy: 0.8064 - val_loss: 0.7178\n","Epoch 115/120\n","\n","Epoch 115: val_accuracy did not improve from 0.81760\n","351/351 - 40s - 114ms/step - accuracy: 0.7904 - loss: 0.7319 - val_accuracy: 0.7956 - val_loss: 0.7414\n","Epoch 116/120\n","\n","Epoch 116: val_accuracy did not improve from 0.81760\n","351/351 - 0s - 1ms/step - accuracy: 0.8203 - loss: 0.7031 - val_accuracy: 0.7936 - val_loss: 0.7413\n","Epoch 117/120\n","\n","Epoch 117: val_accuracy did not improve from 0.81760\n","351/351 - 27s - 78ms/step - accuracy: 0.7921 - loss: 0.7329 - val_accuracy: 0.8086 - val_loss: 0.7069\n","Epoch 118/120\n","\n","Epoch 118: val_accuracy did not improve from 0.81760\n","351/351 - 0s - 1ms/step - accuracy: 0.7891 - loss: 0.7899 - val_accuracy: 0.8096 - val_loss: 0.6971\n","Epoch 119/120\n","\n","Epoch 119: val_accuracy did not improve from 0.81760\n","351/351 - 28s - 79ms/step - accuracy: 0.7939 - loss: 0.7286 - val_accuracy: 0.8074 - val_loss: 0.7108\n","Epoch 120/120\n","\n","Epoch 120: val_accuracy did not improve from 0.81760\n","351/351 - 1s - 2ms/step - accuracy: 0.7891 - loss: 0.7058 - val_accuracy: 0.8054 - val_loss: 0.7095\n"]}]},{"cell_type":"code","source":["#loading_the_best_model:\n","model.load_weights(\"/content/best_model.keras\")"],"metadata":{"id":"_DTlrDgzR0ms","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"error","timestamp":1725027136381,"user_tz":300,"elapsed":228,"user":{"displayName":"Lucas Miguel Iturriago Salas","userId":"11859793377054810664"}},"outputId":"b75fe656-848f-4f2d-95ca-544c11970c79"},"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-b522528d4437>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#loading_the_best_model:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/best_model.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":["#Plotting the accuracy:\n","plt.plot(history.history[\"accuracy\"],label=\"Training Accuracy\")\n","plt.plot(history.history[\"val_accuracy\"],label=\"Validation Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy Training Vs Accuracy Validation\")\n","plt.legend(loc=\"best\")"],"metadata":{"id":"iSrcjpqk1jZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plotting the loss:\n","plt.plot(history.history[\"loss\"],label=\"Training loss\")\n","plt.plot(history.history[\"val_loss\"],label=\"Validation loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss Training Vs Loss Validation\")\n","plt.legend(loc=\"best\")"],"metadata":{"id":"jUUt5SrK1_Ts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.evaluate(x_test,y_test)"],"metadata":{"id":"jAs4unPt2i7-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Obviously the model will be the more optimus if I put many many epochs**"],"metadata":{"id":"k19YVNWL2ibO"}}]}